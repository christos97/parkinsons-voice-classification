% Chapter 6: Results

\chapter{Results}
\label{ch:results}

\section{Overview}

This chapter presents the classification results across all experimental conditions. Results are organized by:

\begin{enumerate}
    \item \textbf{Condition-level summaries} (2$\times$2 factorial)
    \item \textbf{Model comparisons} within each condition
    \item \textbf{Feature ablation analysis} (baseline vs extended)
    \item \textbf{Class weighting analysis}
\end{enumerate}

\section{Summary of Best Results}

\subsection{Dataset A (MDVR-KCL) --- Best Performance}

\begin{table}[H]
\centering
\begin{tabular}{@{}llllp{3.5cm}@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Model} & \textbf{Task} & \textbf{Condition} \\
\midrule
ROC-AUC & $0.857 \pm 0.171$ & Random Forest & Spontaneous & Extended / Unweighted \\
Accuracy & $82.2\% \pm 16.6\%$ & Random Forest & ReadText & Extended / Unweighted \\
\bottomrule
\end{tabular}
\caption{Best performance on Dataset A}
\label{tab:best-results}
\end{table}

\subsection{Key Finding}

\begin{quote}
\textbf{Extended features (78) consistently improved performance} compared to baseline features (47). The highest ROC-AUC of \textbf{0.857} was achieved using the Extended feature set on the Spontaneous Dialogue task.
\end{quote}

\subsection{Dataset B (Benchmark Comparison)}

\textbf{Performance Summary:} Dataset B achieved substantially higher metrics across all classifiers, as shown in Table~\ref{tab:datasetb-results}. These results should be interpreted cautiously given the unknown subject overlap across samples ($n=752$ recordings, subject IDs unavailable).

\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{ROC-AUC} & \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} \\
\midrule
Logistic Regression & $0.867 \pm 0.029$ & $0.828 \pm 0.008$ & $0.885 \pm 0.006$ & $0.890 \pm 0.016$ \\
SVM (RBF) & $0.885 \pm 0.025$ & $0.851 \pm 0.024$ & $0.908 \pm 0.015$ & $0.986 \pm 0.019$ \\
\textbf{Random Forest} & $\mathbf{0.940 \pm 0.013}$ & $\mathbf{0.882 \pm 0.019}$ & $\mathbf{0.925 \pm 0.012}$ & $\mathbf{0.980 \pm 0.015}$ \\
\bottomrule
\end{tabular}
\caption{Dataset B performance using baseline (unweighted) configuration. Note the substantially lower variance compared to Dataset A.}
\label{tab:datasetb-results}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_imp_datasetb_rf.png}
    \caption{Random Forest feature importance (Dataset B). The top features are dominated by advanced signal processing metrics often unavailable in standard clinical settings.}
    \label{fig:datasetb-rf-main}
\end{figure}

\textbf{Key Differences from Dataset A:}
\begin{itemize}
    \item \textbf{Sample Size Effect:} $n=752$ vs $n=37$ reduces variance by an order of magnitude ($\pm 0.01$ vs $\pm 0.15$).
    \item \textbf{Optimistic Bias:} Results may be inflated due to unknown subject overlap across training/test folds.
    \item \textbf{Feature Availability:} Many top-ranked features (e.g., DFA, RPDE) require specialized analysis not captured in Dataset A's baseline features.
\end{itemize}

\section{Condition 1: Baseline Features + Unweighted}

\textbf{Configuration:} 47 features, no class weighting

\subsection{Task: ReadText}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{ROC-AUC} & \textbf{Accuracy} & \textbf{F1} \\
\midrule
Logistic Regression & $0.717 \pm 0.139$ & $0.621 \pm 0.058$ & $0.542 \pm 0.099$ \\
SVM (RBF) & $0.614 \pm 0.312$ & $0.621 \pm 0.106$ & $0.333 \pm 0.333$ \\
Random Forest & $0.590 \pm 0.302$ & $0.629 \pm 0.178$ & $0.351 \pm 0.363$ \\
\bottomrule
\end{tabular}
\caption{Condition 1 --- ReadText results}
\label{tab:c1-readtext}
\end{table}

\subsection{Task: Spontaneous Dialogue}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{ROC-AUC} & \textbf{Accuracy} & \textbf{F1} \\
\midrule
Logistic Regression & $0.760 \pm 0.214$ & $0.639 \pm 0.160$ & $0.539 \pm 0.321$ \\
SVM (RBF) & $0.407 \pm 0.309$ & $0.636 \pm 0.135$ & $0.400 \pm 0.253$ \\
\textbf{Random Forest} & $\mathbf{0.828 \pm 0.148}$ & $\mathbf{0.721 \pm 0.176}$ & $\mathbf{0.567 \pm 0.365}$ \\
\bottomrule
\end{tabular}
\caption{Condition 1 --- Spontaneous Dialogue results}
\label{tab:c1-spontaneous}
\end{table}

\subsection{Observations}

\begin{itemize}
    \item \textbf{Task Difference:} Spontaneous Dialogue yields significantly better separation than ReadText for Random Forest (0.828 vs 0.590) with baseline features.
    \item \textbf{Model Stability:} Logistic Regression is relatively stable across tasks (0.717--0.760).
    \item \textbf{Variance:} High standard deviations ($\pm$0.15--0.30) reflect the small sample size ($n < 40$).
\end{itemize}

\section{Condition 2: Extended Features + Unweighted}

\textbf{Configuration:} 78 features, no class weighting

\subsection{Task: ReadText}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{ROC-AUC} & \textbf{Accuracy} & \textbf{F1} \\
\midrule
Logistic Regression & $0.698 \pm 0.132$ & $0.596 \pm 0.079$ & $0.475 \pm 0.106$ \\
\textbf{SVM (RBF)} & $\mathbf{0.834 \pm 0.153}$ & $0.786 \pm 0.181$ & $0.634 \pm 0.386$ \\
\textbf{Random Forest} & $\mathbf{0.822 \pm 0.166}$ & $0.818 \pm 0.140$ & $0.746 \pm 0.207$ \\
\bottomrule
\end{tabular}
\caption{Condition 2 --- ReadText results}
\label{tab:c2-readtext}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_imp_readtext_rf.png}
    \caption{Random Forest feature importance for ReadText task (Extended features). Fundamental frequency ($F_0$) statistics appear highly predictive.}
    \label{fig:imp-readtext-rf}
\end{figure}

\subsection{Task: Spontaneous Dialogue}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{ROC-AUC} & \textbf{Accuracy} & \textbf{F1} \\
\midrule
Logistic Regression & $0.783 \pm 0.139$ & $0.671 \pm 0.199$ & $0.530 \pm 0.377$ \\
SVM (RBF) & $0.460 \pm 0.294$ & $0.636 \pm 0.089$ & $0.428 \pm 0.258$ \\
\textbf{Random Forest} & $\mathbf{0.857 \pm 0.171}$ & $0.779 \pm 0.161$ & $0.605 \pm 0.387$ \\
\bottomrule
\end{tabular}
\caption{Condition 2 --- Spontaneous Dialogue results}
\label{tab:c2-spontaneous}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_imp_spontaneous_rf.png}
    \caption{Random Forest feature importance for Spontaneous Dialogue task (Extended features). MFCC features show increased importance compared to ReadText.}
    \label{fig:imp-spontaneous-rf}
\end{figure}

\subsection{Observations}

\begin{itemize}
    \item \textbf{Extended Features Impact:} Massive improvement for ReadText task. Random Forest improved from 0.590 to 0.822 (+23pp), and SVM from 0.614 to 0.834 (+22pp).
    \item \textbf{Spontaneous Stability:} Spontaneous Dialogue performance improved slightly (0.828 $\rightarrow$ 0.857) but was already high.
    \item \textbf{SVM Anomaly:} SVM performs excellently on ReadText (0.834) but poorly on Spontaneous Dialogue (0.460), suggesting task-specific feature distribution effects.
\end{itemize}

\section{Feature Ablation Analysis}

\subsection{ROC-AUC Improvement from Feature Extension (ReadText)}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Baseline (47)} & \textbf{Extended (78)} & \textbf{$\Delta$ ROC-AUC} \\
\midrule
Logistic Regression & 0.717 & 0.698 & $-0.019$ \\
SVM (RBF) & 0.614 & 0.834 & $\mathbf{+0.220}$ \\
Random Forest & 0.590 & 0.822 & $\mathbf{+0.232}$ \\
\bottomrule
\end{tabular}
\caption{Feature ablation --- ReadText}
\label{tab:ablation-readtext}
\end{table}

\subsection{ROC-AUC Improvement from Feature Extension (Spontaneous)}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Baseline (47)} & \textbf{Extended (78)} & \textbf{$\Delta$ ROC-AUC} \\
\midrule
Logistic Regression & 0.760 & 0.783 & $+0.023$ \\
SVM (RBF) & 0.407 & 0.460 & $+0.053$ \\
Random Forest & 0.828 & 0.857 & $+0.029$ \\
\bottomrule
\end{tabular}
\caption{Feature ablation --- Spontaneous Dialogue}
\label{tab:ablation-spontaneous}
\end{table}

\textbf{Key Finding:} Feature extension was critical for the ReadText task, rescuing performance from near-chance levels (0.59) to competitive levels (0.82).

\subsection{Analysis}

The dramatic improvement for tree-based models on ReadText (+22--23pp) suggests that:
\begin{enumerate}
    \item Extended spectral features (MFCC delta-delta, spectral shape descriptors) capture task-specific patterns absent in baseline features.
    \item Logistic Regression's slight degradation ($-1.9$pp) may reflect overfitting to the additional 31 dimensions given the small sample size.
    \item Spontaneous Dialogue was already well-represented by baseline features, showing diminishing returns from feature expansion.
\end{enumerate}

\section{Class Weighting Analysis}

Class weighting (\texttt{class\_weight='balanced'}) was evaluated to address Dataset A's mild class imbalance (HC:PD $\approx$ 1.3:1). Results showed minimal improvement and occasional degradation.

\subsection{ReadText Task --- Baseline Features}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Unweighted} & \textbf{Weighted} & \textbf{$\Delta$ ROC-AUC} \\
\midrule
Logistic Regression & $0.717 \pm 0.139$ & $0.717 \pm 0.139$ & $0.000$ \\
Random Forest & $0.590 \pm 0.302$ & $0.687 \pm 0.258$ & $\mathbf{+0.097}$ \\
SVM (RBF) & $0.614 \pm 0.312$ & $0.542 \pm 0.312$ & $-0.072$ \\
\bottomrule
\end{tabular}
\caption{Class weighting impact on ReadText (baseline features). Random Forest showed modest improvement.}
\label{tab:weighting-readtext-baseline}
\end{table}

\subsection{Extended Features --- All Tasks}

\textbf{Observation:} Class weighting provided negligible benefit when using extended features. The best-performing models (RF on Extended/Unweighted) achieved:
\begin{itemize}
    \item ReadText: $0.822 \pm 0.166$ (unweighted) vs $0.805 \pm 0.182$ (weighted) $\rightarrow$ $-1.7$pp
    \item Spontaneous: $0.857 \pm 0.171$ (unweighted) vs $0.823 \pm 0.209$ (weighted) $\rightarrow$ $-3.4$pp
\end{itemize}

\textbf{Interpretation:} The mild imbalance ratio (1.3:1) did not require reweighting. Extended features and subject-grouped cross-validation provided sufficient robustness.

\section{Precision-Recall Tradeoffs}

Classifiers exhibited distinct precision-recall profiles, critical for understanding clinical deployment scenarios.

\subsection{Random Forest (Extended Features)}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Task} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
ReadText & $0.883 \pm 0.162$ & $0.700 \pm 0.298$ & $0.746 \pm 0.207$ \\
Spontaneous & $0.683 \pm 0.410$ & $0.600 \pm 0.435$ & $0.605 \pm 0.387$ \\
\bottomrule
\end{tabular}
\caption{Random Forest precision-recall profile. ReadText configuration favors precision (fewer false positives).}
\label{tab:precision-recall-rf}
\end{table}

\textbf{Analysis:} ReadText achieved high precision (0.88) at the cost of recall (0.70), suggesting the model is conservative in predicting PD. This profile may be preferable for screening applications where false positives carry higher cost.

\subsection{SVM (RBF Kernel)}

SVM demonstrated high recall across both tasks:
\begin{itemize}
    \item ReadText: Recall $0.567 \pm 0.365$, Precision $0.733 \pm 0.435$
    \item Spontaneous: Recall $0.400 \pm 0.279$, Precision $0.533 \pm 0.361$
\end{itemize}

The high variance reflects SVM's sensitivity to small sample cross-validation partitions.

\section{Confusion Matrix Analysis}

The confusion matrices below aggregate out-of-fold predictions from the extended feature set (78 features, unweighted) configuration across all five cross-validation folds. Each matrix displays the total number of correctly and incorrectly classified subjects across all folds, providing a visual complement to the aggregate metrics reported in earlier sections. True labels appear on the rows; predicted labels on the columns.

\subsection{Dataset A --- ReadText}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig_confusion_readtext.pdf}
    \caption{Aggregated confusion matrices for the ReadText task (Extended features, Grouped 5-Fold CV, $n=37$ subjects). Each cell reports the total out-of-fold count and its percentage of all predictions. These results should be interpreted cautiously given the small sample size.}
    \label{fig:confusion-readtext}
\end{figure}

All three classifiers achieve above-chance PD detection on ReadText. Random Forest and SVM (RBF) reach the strongest true-positive rates, consistent with their ROC-AUC advantage. Logistic Regression exhibits a more conservative prediction profile, contributing to higher precision at the cost of recall. The small subject pool ($n=37$) limits absolute cell counts and increases fold-to-fold variability.

\subsection{Dataset A --- Spontaneous Dialogue}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig_confusion_spontaneous.pdf}
    \caption{Aggregated confusion matrices for the Spontaneous Dialogue task (Extended features, Grouped 5-Fold CV, $n=36$ subjects). This task yielded the highest overall ROC-AUC in the study ($0.857 \pm 0.171$, Random Forest).}
    \label{fig:confusion-spontaneous}
\end{figure}

Random Forest demonstrates improved PD detection on Spontaneous Dialogue compared to ReadText, with a higher true-positive count and fewer false negatives. The SVM (RBF) confusion pattern reflects its high variance: strong in some folds, near-chance in others. These results should be interpreted cautiously given the $n=36$ sample size.

\subsection{Dataset B}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig_confusion_dataset_b.pdf}
    \caption{Aggregated confusion matrices for Dataset B (Stratified 5-Fold CV, $n=756$ recordings). The substantially larger sample size yields well-separated confusion patterns with low variance. Results may be optimistic due to unknown subject overlap across folds.}
    \label{fig:confusion-datasetb}
\end{figure}

All three classifiers achieve markedly better separation on Dataset B, confirming the positive effect of sample size on model stability. Random Forest, the best-performing classifier on this dataset (ROC-AUC $0.940 \pm 0.013$), exhibits very few false negatives relative to true positives. These results should be interpreted cautiously given the unknown subject overlap across training and test folds.

\section{Summary of Findings}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcp{6cm}@{}}
\toprule
\textbf{Hypothesis} & \textbf{Result} & \textbf{Evidence} \\
\midrule
H1: Extended features improve ROC-AUC & \checkmark & +23pp on ReadText (RF) \\
H2: Spontaneous Dialogue yields better detection & \checkmark & 0.857 (Spon) vs 0.822 (Read) max \\
H3: Dataset B values are inflated & \checkmark & 0.940 (B) vs 0.857 (A) \\
H4: RF outperforms LR and SVM & \checkmark & Consistent winner across tasks \\
H5: Class weighting improves performance & $\times$ & Marginal or negative impact \\
\bottomrule
\end{tabular}
\caption{Summary of hypothesis testing}
\label{tab:hypothesis-summary}
\end{table}

\subsection{Key Takeaways}

\begin{enumerate}
    \item \textbf{Feature Engineering Matters:} Extended features rescued ReadText performance from near-chance to clinically relevant levels.
    \item \textbf{Task-Dependent Performance:} Spontaneous speech provided better separation than structured read-aloud tasks.
    \item \textbf{Sample Size Dominates:} Dataset B's 20$\times$ larger sample size (752 vs 37) yielded dramatically lower variance and higher metrics.
    \item \textbf{Precision-Recall Tradeoffs:} ReadText models favored precision; clinical deployment must consider cost asymmetry of false positives vs false negatives.
    \item \textbf{Class Weighting Unnecessary:} Mild imbalance (1.3:1) did not benefit from reweighting when using subject-grouped CV and extended features.
\end{enumerate}
