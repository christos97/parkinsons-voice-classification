% Chapter 8: Limitations and Threats to Validity

\chapter{Limitations and Threats to Validity}
\label{ch:limitations}

\section{Overview}

This chapter provides a transparent assessment of the limitations and potential threats to validity in this research. Acknowledging these constraints is essential for appropriate interpretation of results and identification of future research directions.

\section{Sample Size Limitations}

\subsection{Dataset A: Small Subject Pool}

\begin{table}[H]
\centering
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total subjects (ReadText) & 37 \\
Total subjects (SpontaneousDialogue) & 36 \\
Subjects per test fold & $\sim$7--8 \\
PD subjects (minority) & 15--16 \\
HC subjects (majority) & 21 \\
\bottomrule
\end{tabular}
\caption{Dataset A sample size metrics}
\label{tab:sample-size}
\end{table}

\textbf{Implications:}

\begin{itemize}
    \item High variance in fold-level metrics (std $> 0.15$ common)
    \item Limited statistical power for detecting small effects
    \item Results may not generalize to broader populations
\end{itemize}

\subsection{Effect on Statistical Confidence}

With 37 subjects and 5-fold CV:

\begin{itemize}
    \item Each fold has only $\sim$7--8 test subjects
    \item A single misclassification shifts accuracy by $\sim$13--14\%
    \item Observed standard deviations range from 0.13 to 0.42 across metrics
    \item F1 and recall metrics show particularly high variance (std $> 0.24$)
    \item Some folds produced ROC-AUC $< 0.5$ for SVM, indicating instability
\end{itemize}

\textbf{Mitigation:} Results focus on \textbf{relative comparisons} rather than absolute performance claims, acknowledging that confidence intervals overlap substantially.

\section{Subject Identifier Limitations}

\subsection{Dataset B: Missing Subject IDs}

Dataset B contains 756 samples (192 HC, 564 PD) with no subject identifiers. This creates potential for:

\begin{itemize}
    \item \textbf{Subject leakage:} Same subject's multiple samples in train and test sets
    \item \textbf{Optimistic bias:} Inflated performance estimates due to within-subject correlation
    \item \textbf{Unknown generalization:} Cannot assess new-subject performance
    \item \textbf{Severe class imbalance:} HC:PD ratio of 1:2.94 (compared to 1.3:1 in Dataset A)
\end{itemize}

\begin{quote}
\textbf{Caveat:} Results on Dataset B may be optimistic due to unknown subject overlap across folds. The absence of subject identifiers prevents validation of true out-of-subject generalization. The severe class imbalance (74.6\% PD) further complicates interpretation.
\end{quote}

\subsection{Comparison Limitations}

Direct comparison between Dataset A (grouped CV) and Dataset B (standard CV) is confounded by:

\begin{itemize}
    \item Different CV strategies (Grouped Stratified vs Stratified)
    \item Different sample sizes (37 subjects vs 756 samples)
    \item Different speech tasks (read/spontaneous vs sustained /a/ phonation)
    \item Different feature extraction pipelines (custom Librosa/Parselmouth vs unknown)
    \item Different class imbalance ratios (1.3:1 vs 2.94:1)
    \item Unknown subject overlap in Dataset B (potential data leakage)
\end{itemize}

\section{Feature Extraction Limitations}

\subsection{Deterministic Feature Set}

The feature set was designed a priori based on literature review, not data-driven optimization. Limitations include:

\begin{itemize}
    \item \textbf{Potentially suboptimal features:} Other features may be more discriminative
    \item \textbf{Fixed parameters:} F0 range 75--500 Hz, MFCC n\_coeffs=13, n\_fft=2048, hop\_length=512
    \item \textbf{No feature selection:} All 47 (baseline) or 78 (extended) features used without reduction
    \item \textbf{No feature engineering:} No interaction terms, polynomial features, or domain-specific transformations
\end{itemize}

\subsection{Audio Quality Assumptions}

Feature extraction assumes:

\begin{itemize}
    \item Reasonable signal-to-noise ratio
    \item Consistent recording conditions (Dataset A: clinical examination room, $\sim$500 ms reverberation)
    \item No severe clipping or distortion
    \item Mono audio at 22050 Hz (Dataset A downsampled from 44100 Hz)
\end{itemize}

The MDVR-KCL dataset's smartphone recordings (Motorola Moto G4) may introduce device-specific artifacts. Dataset B's recording conditions are unknown.

\section{Model Limitations}

\subsection{No Hyperparameter Tuning}

All models used sklearn default hyperparameters with only random seed and class weighting as controlled variables:

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Model} & \textbf{Fixed Parameters} \\
\midrule
Logistic Regression & $C=1.0$, max\_iter$=1000$, solver=`lbfgs' \\
SVM (RBF) & $C=1.0$, gamma=`scale' (auto-computed) \\
Random Forest & n\_estimators$=100$, max\_depth=None (unlimited) \\
\bottomrule
\end{tabular}
\caption{Fixed hyperparameters (all experiments)}
\label{tab:fixed-params}
\end{table}

\textbf{Implications:}

\begin{itemize}
    \item Performance may be suboptimal compared to tuned baselines
    \item Results represent out-of-the-box sklearn performance
    \item Hyperparameter tuning might change relative model rankings
    \item Class weighting explored separately (USE\_CLASS\_WEIGHT\_BALANCED flag)
\end{itemize}

\textbf{Rationale:} Nested CV on 37 subjects would lead to extreme variance (inner folds $\sim$5--6 subjects); fixed parameters ensure reproducibility and fair model comparison.

\subsection{Classical ML Only}

This thesis explicitly excludes deep learning. Potential missed opportunities:

\begin{itemize}
    \item End-to-end learning from spectrograms
    \item Transfer learning from speech models
    \item Attention mechanisms for temporal modeling
\end{itemize}

\textbf{Rationale:} Deep learning typically requires larger datasets and offers reduced interpretability.

\section{Methodological Limitations}

\subsection{No External Validation}

All results use internal cross-validation (5-fold) without external test sets. Limitations:

\begin{itemize}
    \item No held-out test set from different data source or collection protocol
    \item No multi-site validation (Dataset A: single hospital; Dataset B: single institution)
    \item Generalization to clinical settings with different recording devices unknown
    \item Temporal stability not assessed (all recordings from single time point per subject)
    \item Cross-language and cross-dialect generalization not evaluated
\end{itemize}

\subsection{Class Imbalance Handling}

Class imbalance was present in both datasets:

\begin{itemize}
    \item Dataset A: HC:PD ratio 1.31:1 (ReadText) and 1.40:1 (SpontaneousDialogue)
    \item Dataset B: HC:PD ratio 1:2.94 (severe imbalance)
\end{itemize}

The USE\_CLASS\_WEIGHT\_BALANCED flag was explored as a mitigation strategy, but:

\begin{itemize}
    \item Did not systematically improve performance across all metrics
    \item May have increased variance in some conditions
    \item Optimal weighting strategy may vary by model and dataset
\end{itemize}

\textbf{Alternative approaches not explored:} SMOTE, undersampling, cost-sensitive learning, or threshold adjustment.

\subsection{Binary Classification Only}

The task is limited to PD vs HC classification. Not addressed:

\begin{itemize}
    \item Disease severity prediction (UPDRS scoring)
    \item Progression monitoring (longitudinal data)
    \item Differential diagnosis (PD vs other movement disorders)
    \item Multi-class classification (HC vs PD vs atypical parkinsonism)
\end{itemize}

\subsection{Single Speech Tasks}

Each task analyzed separately. Not addressed:

\begin{itemize}
    \item Task fusion strategies
    \item Multi-task learning
    \item Optimal task selection
\end{itemize}

\section{Threats to Validity}

\subsection{Internal Validity}

\begin{table}[H]
\centering
\begin{tabular}{@{}llp{5cm}@{}}
\toprule
\textbf{Threat} & \textbf{Status} & \textbf{Mitigation} \\
\midrule
Subject leakage & Controlled (Dataset A) & GroupedStratifiedKFold \\
Subject leakage & Uncontrolled (Dataset B) & No subject IDs available \\
Label noise & Unknown & Assumed correct \\
Feature extraction bugs & Mitigated & Unit tests + manual verification \\
Random seed dependence & Controlled & RANDOM\_SEED=42 (fixed) \\
Data preprocessing errors & Mitigated & Standardized audio normalization \\
\bottomrule
\end{tabular}
\caption{Internal validity threats}
\label{tab:internal-validity}
\end{table}

\subsection{External Validity}

\begin{table}[H]
\centering
\begin{tabular}{@{}llp{5cm}@{}}
\toprule
\textbf{Threat} & \textbf{Status} & \textbf{Mitigation} \\
\midrule
Population bias & Likely & Document dataset demographics \\
Recording variability & Present & Standardized extraction \\
Temporal stability & Unknown & Single recording session \\
\bottomrule
\end{tabular}
\caption{External validity threats}
\label{tab:external-validity}
\end{table}

\section{Summary}

This chapter has transparently documented the limitations of this research. These constraints should inform interpretation of results and guide future work. The prioritization of methodological validity over performance optimization means that reported results, while potentially conservative, are more likely to generalize to real-world applications.
